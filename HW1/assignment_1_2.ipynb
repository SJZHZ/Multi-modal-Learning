{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SJZHZ/Multi-modal-Learning/blob/main/assignment_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuPJi-rq1WEZ"
      },
      "source": [
        "# Assignment 1\n",
        "## [Section 2]\n",
        "This assignment will make you familier with \n",
        "1. loading and preprocessing data using built-in function\n",
        "2. how to construct a simple CNN model\n",
        "3. the training and testing pipeline\n",
        "\n",
        "\n",
        "In this assignment, you might find some tutorials useful, such as https://pytorch.org/tutorials/beginner/basics/intro.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N7OzMyzk1WEc"
      },
      "outputs": [],
      "source": [
        "# Import dependencies.\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xe6caWN_1WEd"
      },
      "outputs": [],
      "source": [
        "# Set up your device \n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PbWR_5nB1WEd"
      },
      "outputs": [],
      "source": [
        "# Set up random seed to 1008. Do not change the random seed.\n",
        "# Yes, these are all necessary when you run experiments!\n",
        "seed = 1008\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3MQkukl1WEd"
      },
      "source": [
        "## 1. Data: MNIST [2 pt]\n",
        "#### Load the MNIST training and test dataset using $\\texttt{torch.utils.data.DataLoader}$ and $\\texttt{torchvision.datasets}$. \n",
        "\n",
        "This dataset consists of images of handwritten digit, and thus the number of classes is 10. The shape of image in MNIST dataset is (28, 28, 1)\n",
        "\n",
        "The normalization parameters we will use is (0.1307, 0.3081)\n",
        "\n",
        "More details please refer to  http://yann.lecun.com/exdb/mnist/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_3sASfx1WEe"
      },
      "source": [
        "### 1.1. Load Training Set [1 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UxI1Bov71WEe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST training set with batch size 128, apply data shuffling and normalization\n",
        "# test_loader = TODO\n",
        "channel_mean = 0.1307\n",
        "channel_std = 0.3081\n",
        "train_data = datasets.FashionMNIST(     # MNIST的源下载不了，换了FashionMNIST\n",
        "    root=\"data\",\n",
        "    train=True,                         # 训练集\n",
        "    download=True,                      # 能检测本地缓存的数据集，下一次就不用下载了\n",
        "    transform=transforms.Compose(       # Compose可以压缩两个变换函数到一个对象中，流式地调用\n",
        "        [transforms.ToTensor(), transforms.Normalize(mean=channel_mean,std=channel_std)])\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size = 128,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpWxQFFx1WEe"
      },
      "source": [
        "### 1.2. Load Test Set [1 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zu55m4_h1WEf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([128, 1, 28, 28])\n",
            "Shape of y: torch.Size([128]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST test set with batch size 128, apply normalization\n",
        "# test_loader = TODO\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,                        # 测试集\n",
        "    #download=True,\n",
        "    transform=transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize(mean=channel_mean,std=channel_std)])\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_data,\n",
        "    batch_size = 128,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "for X, y in test_loader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OspQSBSz1WEf"
      },
      "source": [
        "## 2. Models [3 pts]\n",
        "#### You are going to define two convolutional neural networks which are trained to classify MNIST digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqEs-i9H1WEg"
      },
      "source": [
        "### 2.1. CNN without Batch Norm [2 pts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fH7n7KqZ1WEg"
      },
      "outputs": [],
      "source": [
        "# Fill in the values below that make this network valid for MNIST data\n",
        "# Hint: to make sure these, you may calculate the shape of x of every line in the forward.\n",
        "conv1_in_ch = 1             # TODO\n",
        "# 灰度图只有一个通道\n",
        "conv2_in_ch = 20            # TODO\n",
        "# 卷积：前一层输出通道数=后一层输入通道数\n",
        "fc1_in_features = 50*4*4    # TODO\n",
        "# 全连接层输入是一个“特征向量”（提前展平），卷积层中每个通道的每个位置都对应全连接层中的一个特征\n",
        "fc2_in_features = 500       # TODO\n",
        "# 全连接：前一层输出特征数=后一层输入特征数\n",
        "n_classes = 10              # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QyMaix4F1WEg"
      },
      "outputs": [],
      "source": [
        "class NetWithoutBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetWithoutBatchNorm, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=conv1_in_ch, out_channels=20, kernel_size=5, stride=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=conv2_in_ch, out_channels=50, kernel_size=5, stride=1)\n",
        "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=500)\n",
        "        self.fc2 = nn.Linear(in_features=fc2_in_features, out_features=n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, fc1_in_features) # reshaping\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        # Return the log_softmax of x.\n",
        "        #print(F.softmax(x, dim=1)[0])      # 按行（逐向量）作归一化 √\n",
        "        #print(F.softmax(x, dim=0)[0])      # 按列（逐维度）作归一化 X\n",
        "        return F.log_softmax(x, dim=1)      # TODO\n",
        "        # 把输出归一化为概率"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFKJlgu81WEh"
      },
      "source": [
        "### 2.2. CNN with Batch Norm [1 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mc3rp2YW1WEh"
      },
      "outputs": [],
      "source": [
        "# Fill in the values below that make this network valid for MNIST data\n",
        "# Hint: to make sure these, you may calculate the shape of x of every line in the forward.\n",
        "conv1_bn_size = 20          # TODO\n",
        "# 20个通道，分别对每个通道Batch Norm\n",
        "conv2_bn_size = 50          # TODO\n",
        "# 50个通道，分别对每个通道Batch Norm\n",
        "fc1_bn_size = 500           # TODO\n",
        "# 500个特征，分别对每个特征Batch Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b-0DOwV31WEh"
      },
      "outputs": [],
      "source": [
        "# Define the CNN with architecture explained in Part 2.2\n",
        "class NetWithBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetWithBatchNorm, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=conv1_in_ch, out_channels=20, kernel_size=5, stride=1)\n",
        "        self.conv1_bn = nn.BatchNorm2d(conv1_bn_size)\n",
        "        self.conv2 = nn.Conv2d(in_channels=conv2_in_ch, out_channels=50, kernel_size=5, stride=1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(conv2_bn_size)\n",
        "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=500)\n",
        "        self.fc1_bn = nn.BatchNorm1d(fc1_bn_size)\n",
        "        self.fc2 = nn.Linear(in_features=fc2_in_features, out_features=n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1_bn(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, fc1_in_features)\n",
        "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Return the log_softmax of x.\n",
        "        return F.log_softmax(x, dim=1)      # TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0psJkBxm1WEh"
      },
      "source": [
        "## 3. Training & Evaluation [4 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFY-_pb71WEi"
      },
      "source": [
        "### 3.1. Define training method [1 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5xL7MVll1WEi"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval = 100):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    # Loop through data points\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    \n",
        "        # Send data and target to device\n",
        "        x = data.to(device)\n",
        "        y = target.to(device)       # 整数，标签表示类别\n",
        "        # TODO\n",
        "        \n",
        "        # Zero out the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # TODO\n",
        "        \n",
        "        # Pass data through model\n",
        "        y_pred = model(x)           # 10维“概率”向量\n",
        "        # TODO\n",
        "        \n",
        "        # Compute the negative log likelihood loss\n",
        "        loss = F.nll_loss(y_pred, y)\n",
        "        # nll_loss接受一组预测向量和一组标签，返回一个数值\n",
        "        # 组内每行（对应一个数据）：按标签指定的下标取出预测向量的对应维度分量，并取反\n",
        "        # 一列即一组（对应一个batch）：取上述结果的平均值并返回\n",
        "        # TODO\n",
        "        \n",
        "        # Backpropagate loss\n",
        "        loss.backward()\n",
        "        # TODO\n",
        "        \n",
        "        # Make a step with the optimizer\n",
        "        optimizer.step()\n",
        "        # TODO\n",
        "    \n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZygvwvvW1WEi"
      },
      "source": [
        "### 3.2. Define test method [1 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xix5BhkV1WEi"
      },
      "outputs": [],
      "source": [
        "# Define test method\n",
        "def test(model, device, test_loader):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Variable for the total loss \n",
        "    test_loss = 0\n",
        "    # Counter for the correct predictions\n",
        "    num_correct = 0\n",
        "    \n",
        "    # don't need autograd for eval\n",
        "    with torch.no_grad():\n",
        "        # Loop through data points\n",
        "        for data, target in test_loader:\n",
        "\n",
        "            # Send data to device\n",
        "            x = data.to(device)\n",
        "            y = target.to(device)\n",
        "            # TODO\n",
        "            \n",
        "            # Pass data through model\n",
        "            y_pred = model(x)\n",
        "            # TODO\n",
        "            \n",
        "            # Compute the negative log likelihood loss with reduction='sum' and add to total test_loss\n",
        "            test_loss += F.nll_loss(y_pred, y, reduction='sum')\n",
        "            # TODO\n",
        "            \n",
        "            # Get predictions from the model for each data point\n",
        "            pred = y_pred.argmax(1)             # 逐行取argmax\n",
        "            # TODO\n",
        "            \n",
        "            # Add number of correct predictions to total num_correct\n",
        "            num_correct += (pred == y).sum()\n",
        "            # TODO\n",
        "    \n",
        "    # Compute the average test_loss\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)    # TODO\n",
        "    \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
        "        100. * num_correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li3a435x1WEi"
      },
      "source": [
        "### 3.3 Train NetWithoutBatchNorm() [1 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3J_u4HI41WEj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.294814\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.769793\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.929213\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.767682\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.567356\n",
            "\n",
            "Test set: Average loss: 0.6528, Accuracy: 7491/10000 (75%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.677437\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.502804\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.490275\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.563228\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.401109\n",
            "\n",
            "Test set: Average loss: 0.4984, Accuracy: 8250/10000 (82%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.489127\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.429234\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.441836\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.454155\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.496093\n",
            "\n",
            "Test set: Average loss: 0.4518, Accuracy: 8408/10000 (84%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.339381\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.561791\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.585974\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.385948\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.559431\n",
            "\n",
            "Test set: Average loss: 0.4128, Accuracy: 8535/10000 (85%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.455141\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.343481\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.500416\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.399767\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.376723\n",
            "\n",
            "Test set: Average loss: 0.4104, Accuracy: 8514/10000 (85%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.377458\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.372209\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.257373\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.381735\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.320689\n",
            "\n",
            "Test set: Average loss: 0.3783, Accuracy: 8636/10000 (86%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.280681\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.401412\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.310661\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.253938\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.346620\n",
            "\n",
            "Test set: Average loss: 0.3585, Accuracy: 8712/10000 (87%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.378839\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.185994\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.287099\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.293088\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.354072\n",
            "\n",
            "Test set: Average loss: 0.3440, Accuracy: 8738/10000 (87%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.284692\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.273347\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.430426\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.294171\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.409404\n",
            "\n",
            "Test set: Average loss: 0.3444, Accuracy: 8755/10000 (88%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.327276\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.361600\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.298136\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.290244\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.418277\n",
            "\n",
            "Test set: Average loss: 0.3738, Accuracy: 8586/10000 (86%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Deifne model and sent to device\n",
        "model = NetWithoutBatchNorm().to(device)                                    # TODO\n",
        "\n",
        "# Optimizer: SGD with learning rate of 1e-2 and momentum of 0.5\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.5)      # TODO\n",
        "\n",
        "# Training loop with 10 epochs\n",
        "for epoch in range(1, 10 + 1):\n",
        "\n",
        "    # Train model\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    # TODO\n",
        "\n",
        "    # Test model\n",
        "    test(model, device, test_loader)\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRCI-zz_1WEj"
      },
      "source": [
        "### 3.4 Train NetWithBatchNorm() [1 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jQtHCqEq1WEj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.404248\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.629383\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.519546\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.368294\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.344012\n",
            "\n",
            "Test set: Average loss: 0.3746, Accuracy: 8655/10000 (87%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.477129\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.286136\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.375991\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.248755\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.276909\n",
            "\n",
            "Test set: Average loss: 0.3288, Accuracy: 8803/10000 (88%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.311284\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.253042\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.207542\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.250700\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.250815\n",
            "\n",
            "Test set: Average loss: 0.2964, Accuracy: 8942/10000 (89%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.292268\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.200560\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.203393\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.224462\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.201862\n",
            "\n",
            "Test set: Average loss: 0.3191, Accuracy: 8839/10000 (88%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.254581\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.147536\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.260105\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.204480\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.211612\n",
            "\n",
            "Test set: Average loss: 0.2936, Accuracy: 8907/10000 (89%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.248832\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.171477\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.152321\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.168895\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.201433\n",
            "\n",
            "Test set: Average loss: 0.3118, Accuracy: 8834/10000 (88%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.165105\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.313968\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.225592\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.229006\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.151282\n",
            "\n",
            "Test set: Average loss: 0.2659, Accuracy: 9060/10000 (91%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.158737\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.177320\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.195789\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.106417\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.140231\n",
            "\n",
            "Test set: Average loss: 0.2620, Accuracy: 9091/10000 (91%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.129990\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.099295\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.139025\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.222846\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.097708\n",
            "\n",
            "Test set: Average loss: 0.2850, Accuracy: 8993/10000 (90%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.097469\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.157697\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.122628\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.222824\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.107067\n",
            "\n",
            "Test set: Average loss: 0.2583, Accuracy: 9101/10000 (91%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Deifne model and sent to device\n",
        "model = NetWithBatchNorm().to(device)                                       # TODO\n",
        "\n",
        "# Optimizer: SGD with learning rate of 1e-2 and momentum of 0.5\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.5)      # TODO\n",
        "\n",
        "# Training loop with 10 epochs\n",
        "for epoch in range(1, 10 + 1):\n",
        "    \n",
        "    # Train model\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    # TODO\n",
        "    \n",
        "    # Test model\n",
        "    test(model, device, test_loader)\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb9wYwme1WEj"
      },
      "source": [
        "## 4. Empirically, which of the models achieves higher accuracy faster? [1pt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlY11BPA1WEj"
      },
      "source": [
        "Answer: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VoYYcWH5EH0y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "__main__.NetWithBatchNorm"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NetWithBatchNorm"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
