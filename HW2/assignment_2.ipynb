{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "00iMtrQaVkW-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SJZHZ/Multi-modal-Learning/blob/main/Copy_of_%E2%80%9Cassignment_2_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "You will tackle with a sentiment classification task using LSTM model and attention mechanism in this assigment."
      ],
      "metadata": {
        "id": "00iMtrQaVkW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies\n",
        "Please make sure that you are using **GPU** to accelarate computation.\n",
        "\n",
        "Colab FAQ: https://research.google.com/colaboratory/faq.html"
      ],
      "metadata": {
        "id": "GS0Y5IA2T58y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dependencies"
      ],
      "metadata": {
        "id": "KVAMnQR8xflq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import collections\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6Kk2dEh8--MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your device \n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "\n",
        "# The assertion is to make sure GPU is available\n",
        "assert cuda == True"
      ],
      "metadata": {
        "id": "MdjDF7WRF7UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up random seed to 1008. Do not change the random seed.\n",
        "# Yes, these are all necessary when you run experiments!\n",
        "seed = 1008\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n"
      ],
      "metadata": {
        "id": "wXkKqcxiO6b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "The script below will download the required sentiment analysis data.\n",
        "\n",
        "Data folder will be visible in the Colab file-explorer pane, which is loacted at left side of the page.\n"
      ],
      "metadata": {
        "id": "LLvOvdMbO9Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1jqYJ9jhjukhXvEk4GnMAPYE-SvhSG24i' -O data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "SxNT5mN-rHGD",
        "outputId": "017c89aa-66bc-4ffe-bdc7-52d4c5e6997f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 18:54:09--  https://docs.google.com/uc?export=download&id=1jqYJ9jhjukhXvEk4GnMAPYE-SvhSG24i\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.194.101, 172.217.194.100, 172.217.194.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.194.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-4g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fttq9uu5fkudr8i3a8cg58af84dqe56d/1668452025000/14455349759236007028/*/1jqYJ9jhjukhXvEk4GnMAPYE-SvhSG24i?e=download&uuid=8ed91f20-9d4a-4122-899c-4d80fe9ebf39 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-11-14 18:54:13--  https://doc-10-4g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fttq9uu5fkudr8i3a8cg58af84dqe56d/1668452025000/14455349759236007028/*/1jqYJ9jhjukhXvEk4GnMAPYE-SvhSG24i?e=download&uuid=8ed91f20-9d4a-4122-899c-4d80fe9ebf39\n",
            "Resolving doc-10-4g-docs.googleusercontent.com (doc-10-4g-docs.googleusercontent.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to doc-10-4g-docs.googleusercontent.com (doc-10-4g-docs.googleusercontent.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2927295 (2.8M) [application/x-zip-compressed]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.79M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-11-14 18:54:13 (168 MB/s) - ‘data.zip’ saved [2927295/2927295]\n",
            "\n",
            "Archive:  data.zip\n",
            "  inflating: sentiment/mapping.txt   \n",
            "  inflating: sentiment/test_labels.txt  \n",
            "  inflating: sentiment/test_text.txt  \n",
            "  inflating: sentiment/train_labels.txt  \n",
            "  inflating: sentiment/train_text.txt  \n",
            "  inflating: sentiment/val_labels.txt  \n",
            "  inflating: sentiment/val_text.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus\n",
        "Glove will be used as the word embedding tool in this assigment."
      ],
      "metadata": {
        "id": "7teo5qvRPnYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "34dba6GT_EhP",
        "outputId": "76336930-6277-4b15-91f7-71c6becdea09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 18:54:21--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-11-14 18:54:21--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-11-14 18:54:22--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 39s  \n",
            "\n",
            "2022-11-14 18:57:02 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess\n",
        "Preprocess data, then construct dataloader and vocabulary."
      ],
      "metadata": {
        "id": "VPEcJ_L8QqZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Glove pretrained word embedding."
      ],
      "metadata": {
        "id": "fXPeeetKXvWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "def LoadGloveModel(gloveFile, dim):    # 从文件中加载Glove预训练模型。返回一个词典和一个矩阵\n",
        "    print(\"Loading Glove Model\")\n",
        "    f = open(gloveFile,'r')\n",
        "    model = {}\n",
        "    array = []\n",
        "    model[0] = 0\n",
        "    array.append([0] * dim)\n",
        "    i = 1\n",
        "    for line in f:\n",
        "        splitLine = line.split()\n",
        "        word = splitLine[0]\n",
        "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
        "        model[word] = i       # 序号\n",
        "        array.append(embedding)    # 嵌入   \n",
        "        i += 1\n",
        "    array = np.array(array)\n",
        "    print(\"Done.\",len(model),\" words loaded!\")\n",
        "    return model, array\n",
        "\n",
        "dim = 300\n",
        "GloveDict, GloveEmbedding = LoadGloveModel(\"glove.6B.300d.txt\", 300)       # 100维\n",
        "print(GloveDict['my'])\n",
        "print(GloveEmbedding[0])"
      ],
      "metadata": {
        "id": "nADNNIT11U-P",
        "outputId": "f0a58417-fadf-47eb-9489-d9d557f4e4d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Glove Model\n",
            "Done. 400001  words loaded!\n",
            "193\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct your own vocabulary without other corpus.\n",
        "Hint: You should construct a vocabulary to map the word to index."
      ],
      "metadata": {
        "id": "hOkdvhoeRHu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "MyWord = collections.Counter()\n",
        "f = open('sentiment'+'/train_text.txt')\n",
        "for line in f:\n",
        "    splitLine = line.split()\n",
        "    MyWord.update(splitLine)    # 更新词频列表\n",
        "print(len(MyWord))\n",
        "MyDict = {}\n",
        "i = 0\n",
        "MyDict[0] = 0\n",
        "for word in MyWord:\n",
        "  if MyWord[word] > 0:      # 超过1次\n",
        "    i += 1\n",
        "    MyDict[word] = i\n",
        "print(len(MyDict))\n",
        "Embedding300 = nn.Embedding(len(MyDict), 300)\n",
        "templist = []\n",
        "for i in range(len(MyDict)):\n",
        "  b = Embedding300(torch.LongTensor([i])).detach().numpy()[0]\n",
        "  templist.append(b)\n",
        "MyEmbedding = np.array(templist)\n",
        "print(MyEmbedding.shape)"
      ],
      "metadata": {
        "id": "FXgxiqptBsAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ea3389-4a66-4690-eb34-43d5f685ac48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106360\n",
            "106361\n",
            "(106361, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data\n",
        "Load data and construct dataloader."
      ],
      "metadata": {
        "id": "zbw-0N7KQ6B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'sentiment'\n",
        "\n",
        "# TODO\n",
        "def LoadText(DataFile):    # 从文件中加载训练文本（每行是一句话）。返回一个列表\n",
        "    print(\"Loading Text\")\n",
        "    f = open(DataFile,'r')\n",
        "    temp_data = []\n",
        "    for line in f:\n",
        "        splitLine = line.split()\n",
        "        temp_data.append(splitLine)\n",
        "    print(\"Done.\",len(temp_data),\" sentences loaded!\")\n",
        "    return temp_data\n",
        "\n",
        "\n",
        "maxlen = 40    # 句子对齐到最大长度\n",
        "def LoadData(Text, Dict, dim):\n",
        "  print(\"Loading Data\")\n",
        "  temp_data = []\n",
        "  for line in Text:\n",
        "    temp_sentence = []\n",
        "    for word in line:\n",
        "      if word in Dict:      # 匹配的才算\n",
        "        temp_sentence.append(Dict[word])\n",
        "    temp_len = len(temp_sentence)\n",
        "    for i in range(maxlen - temp_len): \n",
        "      temp_sentence.append(Dict[0])\n",
        "    temp_data.append(temp_sentence)\n",
        "  print(\"Done.\",len(temp_data),\" lines loaded!\")\n",
        "  return temp_data\n",
        "\n",
        "def LoadLabel(LabelFile):     # 从文件中加载训练标签（每行一个数值）。返回一个列表\n",
        "    print(\"Loading Label\")\n",
        "    f = open(LabelFile,'r')\n",
        "    temp_labels = []\n",
        "    for line in f:\n",
        "        splitLine = line.split()\n",
        "        label = int(splitLine[-1])  # 每行最后一个位,读入的是字符串\n",
        "        label_vec = [0] * 3   # BiRNN模型输出3个类别,需要转换为n维向量\n",
        "        label_vec[label] = 1\n",
        "        temp_labels.append(label_vec)\n",
        "    print(\"Done.\",len(temp_labels),\" lines loaded!\")\n",
        "    return temp_labels\n",
        "\n",
        "def MakeDataset(textname, labelname, dictname, dim):\n",
        "  temp_text = LoadText(textname)\n",
        "  temp_data = LoadData(temp_text, dictname, dim)\n",
        "  temp_label = LoadLabel(labelname)\n",
        "  temp = torch.from_numpy(np.array(temp_label))\n",
        "  print(temp.shape, temp.dtype)\n",
        "  dataset = TensorDataset(torch.from_numpy(np.array(temp_data)),torch.from_numpy(np.array(temp_label)))\n",
        "  return dataset\n",
        "\n",
        "# 根据想用的词典\n",
        "tempdict, tempdim = GloveDict, 300\n",
        "# tempdict, tempdim = MyDict, 300\n",
        "\n",
        "val_Dataset = MakeDataset(data_dir + '/val_text.txt', data_dir + '/val_labels.txt', tempdict, tempdim)\n",
        "test_Dataset = MakeDataset(data_dir + '/test_text.txt', data_dir + '/test_labels.txt', tempdict, tempdim)\n",
        "train_Dataset = MakeDataset(data_dir + '/train_text.txt', data_dir + '/train_labels.txt', tempdict, tempdim)\n",
        "\n",
        "batch_size = 128\n",
        "val_Loader = DataLoader(val_Dataset, shuffle=True, batch_size=batch_size)\n",
        "test_Loader = DataLoader(test_Dataset, shuffle=True, batch_size=batch_size)\n",
        "train_Loader = DataLoader(train_Dataset, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "p5zh-qrS_aAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e197bf5b-65ed-4420-cb72-f108fc9ab9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Text\n",
            "Done. 2000  sentences loaded!\n",
            "Loading Data\n",
            "Done. 2000  lines loaded!\n",
            "Loading Label\n",
            "Done. 2000  lines loaded!\n",
            "torch.Size([2000, 3]) torch.int64\n",
            "Loading Text\n",
            "Done. 12284  sentences loaded!\n",
            "Loading Data\n",
            "Done. 12284  lines loaded!\n",
            "Loading Label\n",
            "Done. 12284  lines loaded!\n",
            "torch.Size([12284, 3]) torch.int64\n",
            "Loading Text\n",
            "Done. 45615  sentences loaded!\n",
            "Loading Data\n",
            "Done. 45615  lines loaded!\n",
            "Loading Label\n",
            "Done. 45615  lines loaded!\n",
            "torch.Size([45615, 3]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "Bidirectional LSTM and attention mechanism will be used in this section."
      ],
      "metadata": {
        "id": "MOfFQoClxPTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Zoo"
      ],
      "metadata": {
        "id": "mDPbq8VmEvdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
        "        super(BiRNN, self).__init__()\n",
        "        if pretrained_embedding is None:\n",
        "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        else:\n",
        "            self.embedding= nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype=torch.float), freeze=True)\n",
        "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        self.decoder = nn.Sequential(nn.Linear(4 * num_hiddens, num_hiddens), nn.Linear(num_hiddens, 3))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        self.encoder.flatten_parameters()\n",
        "        outputs, _ = self.encoder(embeddings)\n",
        "        encoding = torch.cat((outputs[:,0,:], outputs[:,-1,:]), dim=1)\n",
        "        outs = self.decoder(encoding)\n",
        "        return outs"
      ],
      "metadata": {
        "id": "vTyWAUr81oR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiRNN_attention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
        "        super(BiRNN_attention, self).__init__()\n",
        "        if pretrained_embedding is None:\n",
        "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype=torch.float),\n",
        "                                                          freeze=True)\n",
        "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        self.weight_W = nn.Parameter(torch.Tensor(2 * num_hiddens, 2 * num_hiddens))\n",
        "        self.weight_proj = nn.Parameter(torch.Tensor(2 * num_hiddens, 1))\n",
        "\n",
        "        self.decoder = nn.Sequential(nn.Linear(2 * num_hiddens, num_hiddens), nn.Linear(num_hiddens, 3))\n",
        "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        mask = 1 - torch.clamp(inputs, min=0, max=1)\n",
        "        embeddings = self.embedding(inputs)\n",
        "        states, hidden = self.encoder(embeddings.permute([0, 1, 2]))\n",
        "        u = torch.tanh(torch.matmul(states, self.weight_W))\n",
        "        att = torch.matmul(u, self.weight_proj)\n",
        "        att = att + mask.unsqueeze(2) * -1e7\n",
        "        att_score = F.softmax(att, dim=1)\n",
        "        scored_x = states * att_score\n",
        "        encoding = torch.sum(scored_x, dim=1)\n",
        "        outputs = self.decoder(encoding)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "WBXSkgb41oVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "You should train two models above with Glove pretrained word embedding and random initialized word embedding.\n",
        "\n",
        "Evaluation on the validation set and print out accuracy after training one epoch is required.\n",
        "\n",
        "You can tune some parameters and try different techniques, such as learning rate scheduler."
      ],
      "metadata": {
        "id": "1HE235LrErqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, Loader, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for sentence, label in Loader:\n",
        "        optimizer.zero_grad()\n",
        "        sentence = sentence.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        pred = model(sentence).squeeze(1)\n",
        "        loss = criterion(pred, label.float()) # label本来是整数列表\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(Loader)\n",
        "\n",
        "def test(model, Loader, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    for sentence, label in Loader:\n",
        "        sentence = sentence.to(device)\n",
        "        label = label.to(device)\n",
        "        pred = model(sentence).squeeze(1)\n",
        "        loss = criterion(pred, label.float())\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += (pred.argmax(dim=1) == label.argmax(dim=1)).float().mean().cpu().item()\n",
        "        \n",
        "    return epoch_loss / len(Loader), epoch_acc / len(Loader)\n"
      ],
      "metadata": {
        "id": "cEE1dzlIPkMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BiRNN with Glove pretrained word embedding\n",
        "# TODO\n",
        "vocabsize, embedsize = GloveEmbedding.shape[0], GloveEmbedding.shape[1]\n",
        "\n",
        "Model1 = BiRNN(vocab_size=vocabsize, embed_size=embedsize, num_hiddens=20, num_layers=2, pretrained_embedding=GloveEmbedding).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(Model1.parameters())\n",
        "\n",
        "for iter in range(5):    # 写的很烂,次数太多反而准确率变低\n",
        "  train_loss = train(model=Model1, Loader=train_Loader, optimizer=optimizer, criterion=criterion)\n",
        "  test_loss, test_acc = test(model=Model1, Loader=val_Loader, criterion=criterion)\n",
        "  print(iter, \": train \", train_loss, \", val \", test_loss, test_acc)\n",
        "test_loss, test_acc = test(model=Model1, Loader=test_Loader, criterion=criterion)\n",
        "print(\"test loss\", test_loss, \"test acc\", test_acc)"
      ],
      "metadata": {
        "id": "j3gbyRUXEPZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79104e76-f572-49e8-9557-bd924ecd6de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : train  0.9184199730221297 , val  0.8688862584531307 0.5880859382450581\n",
            "1 : train  0.8289449710805877 , val  0.8563771359622478 0.5982421897351742\n",
            "2 : train  0.8048629949406749 , val  0.8530773296952248 0.5984374992549419\n",
            "3 : train  0.7852504493809548 , val  0.8422710783779621 0.6081054694950581\n",
            "4 : train  0.7694343532166895 , val  0.8495704121887684 0.6119140647351742\n",
            "test loss 0.8815583176910877 test acc 0.6035576276481152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BiRNN without pretrained word embedding\n",
        "# TODO\n",
        "vocabsize, embedsize = MyEmbedding.shape[0], MyEmbedding.shape[1]\n",
        "\n",
        "Model2 = BiRNN(vocab_size=vocabsize , embed_size=embedsize, num_hiddens=200, num_layers=2, pretrained_embedding=MyEmbedding).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(Model2.parameters())\n",
        "\n",
        "for iter in range(3):\n",
        "  train_loss = train(model=Model2, Loader=train_Loader, optimizer=optimizer, criterion=criterion)\n",
        "  test_loss, test_acc = test(model=Model2, Loader=val_Loader, criterion=criterion)\n",
        "  print(iter, \": train \", train_loss, \", val \", test_loss, test_acc)\n",
        "test_loss, test_acc = test(model=Model2, Loader=test_Loader, criterion=criterion)\n",
        "print(\"test loss\", test_loss, \"test acc\", test_acc)"
      ],
      "metadata": {
        "id": "6rXFCb56GBOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfc1d4e-52e9-47e4-ed7f-53d95c35a22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : train  0.930183180240022 , val  0.8794585727155209 0.5646484382450581\n",
            "1 : train  0.7869017695178505 , val  0.8441049233078957 0.6104492209851742\n",
            "2 : train  0.6356490883840566 , val  0.8743134699761868 0.6092773452401161\n",
            "test loss 1.0801431400080521 test acc 0.5492612775415182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BiRNN_attention with Glove pretrained embedding\n",
        "# TODO\n",
        "vocabsize, embedsize = GloveEmbedding.shape[0], GloveEmbedding.shape[1]\n",
        "\n",
        "Model3 = BiRNN_attention(vocab_size=vocabsize , embed_size=embedsize, num_hiddens=20, num_layers=2, pretrained_embedding=GloveEmbedding).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(Model3.parameters())\n",
        "\n",
        "for iter in range(5):\n",
        "  train_loss = train(model=Model3, Loader=train_Loader, optimizer=optimizer, criterion=criterion)\n",
        "  test_loss, test_acc = test(model=Model3, Loader=val_Loader, criterion=criterion)\n",
        "  print(iter, \": train \", train_loss, \", val \", test_loss, test_acc)\n",
        "test_loss, test_acc = test(model=Model3, Loader=test_Loader, criterion=criterion)\n",
        "print(\"test loss\", test_loss, \"test acc\", test_acc)"
      ],
      "metadata": {
        "id": "aeetQ2uZGBXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bc8e9d-b1d5-4f85-97f8-fee4abd47752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : train  0.9117326836626068 , val  0.8522169962525368 0.58984375\n",
            "1 : train  0.8178155744443086 , val  0.830701969563961 0.6117187514901161\n",
            "2 : train  0.7954208246466159 , val  0.8367760479450226 0.6081054694950581\n",
            "3 : train  0.7792759522670457 , val  0.8300911784172058 0.6201171875\n",
            "4 : train  0.7627293567697541 , val  0.8295808471739292 0.6142578125\n",
            "test loss 0.9039642363786697 test acc 0.6000582786897818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BiRNN_attention without pretrained word embedding\n",
        "# TODO\n",
        "vocabsize, embedsize = MyEmbedding.shape[0], MyEmbedding.shape[1]\n",
        "\n",
        "Model4 = BiRNN(vocab_size=vocabsize , embed_size=embedsize, num_hiddens=200, num_layers=2, pretrained_embedding=MyEmbedding).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(Model4.parameters())\n",
        "\n",
        "for iter in range(3):\n",
        "  train_loss = train(model=Model4, Loader=train_Loader, optimizer=optimizer, criterion=criterion)\n",
        "  test_loss, test_acc = test(model=Model4, Loader=val_Loader, criterion=criterion)\n",
        "  print(iter, \": train \", train_loss, \", val \", test_loss, test_acc)\n",
        "test_loss, test_acc = test(model=Model4, Loader=test_Loader, criterion=criterion)\n",
        "print(\"test loss\", test_loss, \"test acc\", test_acc)"
      ],
      "metadata": {
        "id": "uiRSuAy0GBdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3001f6-fa45-4f2c-da44-b44a6fd6aaf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : train  0.9329534366685135 , val  0.880100667476654 0.5702148452401161\n",
            "1 : train  0.7887001314750907 , val  0.844881996512413 0.6101562492549419\n",
            "2 : train  0.6334546577362787 , val  0.8882623575627804 0.6125000007450581\n",
            "test loss 1.117300431554516 test acc 0.542000062763691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report (optional)\n",
        "You can briefly report what strategies you attempted in this assignment."
      ],
      "metadata": {
        "id": "Cpjn8StjHcY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 不太明白怎么自己作vocabulary，我是把test data的单词拿来建一个dict，然后做成embedding\n",
        "2. 把单词用dict映成序号，然后使用embedding表达。对于生词统一记为0合适吗？\n",
        "3. 训练写的不太好，随着训练次数增加，val的loss越来越大，应该是发生了过拟合\n",
        "4. 晚上把Colab的免费时长用完了。。。只能开个新号了"
      ],
      "metadata": {
        "id": "spqfhNO3r6oj"
      }
    }
  ]
}